{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using word2vec to see what CBC 'knows' about Canada\n",
    "\n",
    "### or \n",
    "\n",
    "# What happened when I forced my computer to read 135,000 news stories\n",
    "\n",
    "---\n",
    "\n",
    "A talk at PyCon Canada 2019 by Roberto Rocha\n",
    "\n",
    "Investigative data journalist at CBC\n",
    "\n",
    "\n",
    "Twitter/Github: robroc\n",
    "\n",
    "---\n",
    "\n",
    "First things first. I'm not a real data scientist. I'm data journalist who uses some data science tools. I don't do predictive models. My job is not to predict, but to analyze and report. I'm by no means a machine learning practioner. So I have a very shallow understanding of this stuff.\n",
    "\n",
    "What follows are experiments in using a text-based deep learning tool to get insights out of mountains of text. Specifically, as many past articles published on cbc.ca/news that I could get my hands on. And this was about 135,000 stories from 2014 to mid 2019.\n",
    "\n",
    "This notebook is almost a complete copy of a [word2vec tutorial on Kaggle](https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial), with a few modifications. That gentleman deserves most of the credit.\n",
    "\n",
    "Also, I can't share the source data or the trained model. Not yet, anyway. So please don't ask. Sorry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is word2vec? \n",
    "\n",
    "## **Short answer:** \n",
    "### A way to turn words into numbers based on their context and put them in a high-dimensional vector space\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an illustration of a word space reduced to three dimensions by PCA. [Taken from here](https://projector.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vector](images\\word2vec.gif \"3dvector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Shorter answer:**\n",
    "\n",
    "\n",
    "<img src=\"images\\giphy.gif\" alt=\"magic\" style=\"width: 400px;\"/>\n",
    "\n",
    "Yes, this is the most magical thing I've seen a computer do. If you disagree, fight me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it used for?\n",
    "\n",
    "\n",
    "* Recommendation engines\n",
    "* Document similarity\n",
    "* Bias detection\n",
    "\n",
    "\n",
    "A [word2vec model trained on Google News articles](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf) found some signs of bias. I was wondering if CBC would fare better.\n",
    "\n",
    "![bias](images/study.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On to the code. First, load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "import unicodedata\n",
    "import spacy  # For preprocessing\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the CBC articles from a pandas dataframe. There's one article per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Beto\\AppData\\Local\\Continuum\\miniconda3\\envs\\word2vec\\lib\\site-packages\\dateutil\\parser\\_parser.py:1206: UnknownTimezoneWarning: tzname ET identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    }
   ],
   "source": [
    "archive = pd.read_csv('cbc_stories_with_date_2014-19.csv', parse_dates = ['pubdate'])\n",
    "stories = archive.body.tolist()\n",
    "stories = [unicodedata.normalize(\"NFKD\", s).encode('ASCII', 'ignore').decode().strip() for s in stories if pd.notna(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what one story looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The New Brunswick government will have a razor-thin $4.5 million budget surplus this year, Progressive Conservative Finance Minister Ernie Steeves has announced.  The dramatic improvement over the $188.7-million deficit projected in the Liberal government\\'s last budget is attributed mostly to higher than expected revenues, including personal and corporate income taxes and federal transfer payments. The new projection is from the government\\'s third-quarter financial results for 2018-19. Premier Blaine Higgs let the cat out of the bag two weeks ago when he said the province would have a balanced budget this year. But Steeves warned that the surplus is on a \"thin edge\" and the province\\'s debt continues to grow. That means his first provincial budget next month will look for new ways to reduce spending, he said. \"It\\'s going to be a difficult budget. We need to explore new ways to do things in a more efficient manner.\" Steeves said he\\'s looking at all programs \"from beginning to end\" to find savings. Figures released by Steeves\\'s department project corporate income tax revenue will be $130.9 million higher than expected when the 2018-19 year ends on March 31. Personal income tax is forecast to be $101 million higher than expected. Federal transfers for health and social services are also up because Statistics Canada has updated its 2016 census figures to reflect a higher New Brunswick population. Spending is also higher than in the budget, including $21.7 million the province must spend to pay off contractors who were ordered in December to stop their work on various projects. That\\'s when Steeves cut planned infrastructure spending by more than $200 million, cancelling several large projects started under the Gallant Liberals. That included a replacement New Brunswick Museum in Saint John and a new courthouse in Fredericton. In its throne speech last fall, the Higgs government promised two tax cuts whenever the province had a balanced budget. One was the elimination of the small business tax and the other was an end to property taxes on secondary properties.  Steeves said the government hasn\\'t decided whether to make those cuts in its upcoming budget. The minister will introduce his first full budget for the 2019-20 fiscal year when the legislature resumes March 19.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134997"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of stories:\n",
    "len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309522810"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of words:\n",
    "sum([len(txt) for txt in stories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2005        2\n",
       "2007        3\n",
       "2008        1\n",
       "2009        1\n",
       "2011        1\n",
       "2012        3\n",
       "2013        9\n",
       "2014    30002\n",
       "2015    30011\n",
       "2016    30069\n",
       "2017    30202\n",
       "2018      428\n",
       "2019    14811\n",
       "Name: pubdate, dtype: int64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of stories per publicaton year\n",
    "archive.pubdate.dt.year.value_counts(sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pre-processing\n",
    "\n",
    "The cleanup is divided in two parts:\n",
    "    \n",
    "1. Split stories into sentences and remove non-alpha characters\n",
    "2. Remove stop-words (and, I, or, with) and optionally lemmatize tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1\n",
    "\n",
    "Load the SpaCy English model and sentencizer, split stories into sentences, and do a quick cleaning: remove non-alphanumeric character and trailing spaces.\n",
    "\n",
    "The SpaCy sentencizer smartly detects sentences beyond just splitting on periods. This avoids creating sentences out of \"Mr.\" or acronyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to sentensize: 107.93 mins\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser']) \n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "sents = []\n",
    "\n",
    "t = time()\n",
    "\n",
    "for story in stories: \n",
    "    s = [re.sub(\"[^A-zÀ-ú']+\", ' ', sent.string).strip() for sent in nlp(story).sents]\n",
    "    sents.extend(s)\n",
    "\n",
    "print('Time to sentensize: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is was the cleaned up senteces look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At the end of the week one photo submission wins a CBC London prize pack',\n",
       " \"Currently in London it's and cloudy\",\n",
       " \"Today's forecast is mainly cloudy with a percent chance of flurries\",\n",
       " 'Winds are west between and km hr',\n",
       " 'The temperature is steady at with a wind chill near Gas is selling in London between and per litre',\n",
       " 'Source gasbuddy com The Canadian dollar closed yesterday at cents U S The market opens today at a m',\n",
       " \"A report from the Toronto law firm tasked with investigating harassment at the City of London has found that nearly city employees say they've been harassed discriminated against bullied intimidated or experienced reprisal in the workplace\",\n",
       " 'Staff at Rubin Thomlinson LLP sent surveys to city of London employees and received back',\n",
       " \"Of those responses current and former employees confirmed that they'd experienced this type of behaviour\",\n",
       " 'Intimidation was the most common behaviour experienced by city staff followed by bullying and harassment the report found']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2\n",
    "\n",
    "Now we need to clean individual tokens in the sentences: Remove stop-words and lemmatize words that are not a named entity.\n",
    "\n",
    "Lemmatization is controversial. Some say it removes nuances. But for me, I wanted to reduce noise. I didn't care to keep plurals or the conjugated forms of verbs. However, I wanted to maintain some resolution. I didn't want Apple the company to be reduced to apple the fruit. Luckily, SpaCy has a handy named entity recognition function that identifies organizations, proper names and places so they don't get lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 107.92 mins\n"
     ]
    }
   ],
   "source": [
    "def cleaning(doc):\n",
    "    \"\"\"\"Lemmatizes (optional) and removes stopwords\n",
    "    `doc` needs to be a spacy Doc object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lemmatize words only if they're not a recognized entity type and skip stopwords\n",
    "    txt = [token.lemma_ if token.ent_type == 0 else token.text for token in doc if not token.is_stop]\n",
    "    \n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)\n",
    "\n",
    "t = time()\n",
    "\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(sents, batch_size=5000, n_threads=-1)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the lemmatized and refined sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at end week photo submission win CBC London prize pack',\n",
       " 'currently London be cloudy',\n",
       " \"Today 's forecast mainly cloudy percent chance flurry\",\n",
       " 'wind west km hr',\n",
       " 'the temperature steady wind chill near Gas sell London litre',\n",
       " 'source gasbuddy com the Canadian dollar close yesterday cent U S the market open today m',\n",
       " 'a report Toronto law firm task investigate harassment City London find nearly city employee have harass discriminate bully intimidate experience reprisal workplace',\n",
       " 'staff Rubin Thomlinson LLP send survey city London employee receive',\n",
       " 'of response current employee confirm would experience type behaviour',\n",
       " 'intimidation common behaviour experience city staff follow bully harassment report find']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the sentences through gensim's phraser. This is useful to catch bigrams (two-word combos that go together frequently) that mean different things.\n",
    "\n",
    "For example, we'll want to differentiate Justin Trudeau from Trudeau airport, or Doug Ford from Rob Ford.\n",
    "\n",
    "Finally, we want to split sentences into their component words. That's what wod2vec needs as input: a list of lists of words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "# More info: https://radimrehurek.com/gensim/models/phrases.html\n",
    "\n",
    "token_lists = [line.split() for line in txt if line]\n",
    "phrases = Phrases(token_lists, min_count=20, progress_per=10000)\n",
    "\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[token_lists]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the final result: a list of words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at',\n",
       " 'end',\n",
       " 'week',\n",
       " 'photo',\n",
       " 'submission',\n",
       " 'win',\n",
       " 'CBC',\n",
       " 'London',\n",
       " 'prize',\n",
       " 'pack']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.corpus[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize word2vec, keeping most default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "w2v_model = Word2Vec(min_count = 10,\n",
    "                     window = 5,\n",
    "                     size = 200,\n",
    "                     workers=cores-1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustraton of the `window` parameter: how many neighbouring words to consider for the context. In this case, the window size is 2.\n",
    "\n",
    "Source: [Chis McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![window](images/training_data.png \"window\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model (finally!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples = w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the models for later. In this case, it's a KeyedVetor format that loads faster and uses less memory if I'm done training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.save('word2vec5.wv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "Now we can test how good the model is. A word like Toronto should be most similar to other large Canadian cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Montreal', 0.7378078103065491),\n",
       " ('Hamilton', 0.725394070148468),\n",
       " ('Vancouver', 0.7078638076782227),\n",
       " ('London', 0.6668410301208496),\n",
       " ('Ottawa', 0.663960337638855),\n",
       " ('Calgary', 0.640132486820221),\n",
       " ('Edmonton', 0.6318969130516052),\n",
       " ('Winnipeg', 0.6047239899635315),\n",
       " ('windsor', 0.5858883261680603),\n",
       " ('Guelph', 0.55284583568573)]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('Toronto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it should also know that American cities are more similar to other American cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Philadelphia', 0.7164496779441833),\n",
       " ('Boston', 0.6958774328231812),\n",
       " ('Los_Angeles', 0.685975193977356),\n",
       " ('Pittsburgh', 0.6837000846862793),\n",
       " ('St_Louis', 0.6808935403823853),\n",
       " ('Nashville', 0.6802417039871216),\n",
       " ('Atlanta', 0.6772310733795166),\n",
       " ('Dallas', 0.6682097911834717),\n",
       " ('Seattle', 0.6489051580429077),\n",
       " ('Tampa', 0.617918848991394)]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('Chicago')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test other words that you know are similar. They should have high cosine similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('surgeon', 'doctor') 0.6474745\n",
      "('Canada', 'country') 0.7405826\n",
      "('blue', 'red') 0.6960444\n",
      "('movie', 'film') 0.8581524\n",
      "('dog', 'cat') 0.7754835\n",
      "('Liberal', 'Conservative') 0.7897476\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [\n",
    "    ('surgeon', 'doctor'),\n",
    "    ('Canada', 'country'),\n",
    "    ('blue', 'red'),\n",
    "    ('movie', 'film'),\n",
    "    ('dog', 'cat'),\n",
    "    ('Liberal', 'Conservative')\n",
    "]\n",
    "\n",
    "for pair in word_pairs:\n",
    "    print(pair, w2v_model.wv.similarity(*pair) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the named entity recognizer did its job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('strawberry', 0.6267320513725281),\n",
       " ('sweet_potato', 0.6007342338562012),\n",
       " ('potato', 0.5943686366081238),\n",
       " ('broccoli', 0.5879802703857422),\n",
       " ('fruit', 0.5761351585388184),\n",
       " ('citrus', 0.5758861303329468),\n",
       " ('carrot', 0.5754052400588989),\n",
       " ('caramel', 0.5738547444343567),\n",
       " ('pecan', 0.5734065771102905),\n",
       " ('pc_organics', 0.5699262619018555)]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple_Inc', 0.7471168041229248),\n",
       " ('Microsoft', 0.7063766717910767),\n",
       " ('Samsung', 0.6787918210029602),\n",
       " ('BlackBerry', 0.6776144504547119),\n",
       " ('iPhone', 0.6532427072525024),\n",
       " ('Apple_Google', 0.6327992677688599),\n",
       " ('Google', 0.6271075010299683),\n",
       " ('Android', 0.6246681809425354),\n",
       " ('iPhones', 0.5998338460922241),\n",
       " ('Amazon', 0.5953640341758728)]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('Apple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies!\n",
    "\n",
    "This is the coolest feature of word2vec. Since words were converted to numbers, we can do math with them. We can, for example, subtract one word from a pair of words and get the semantic equivalent.\n",
    "\n",
    "In other words we can as questions like: what is to woman as king is to man?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.4101138710975647),\n",
       " ('prince', 0.40189918875694275),\n",
       " ('William_Kate', 0.39393043518066406),\n",
       " ('princess', 0.37827563285827637),\n",
       " ('emperor', 0.36916765570640564),\n",
       " ('king_Salman', 0.36641332507133484),\n",
       " ('connolly', 0.35293644666671753),\n",
       " ('Kertesz', 0.35092097520828247),\n",
       " ('luther', 0.3470686078071594),\n",
       " ('pope', 0.3422856330871582)]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a corpus of Canadian news, let's see what CBC knows about Canadian cities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Toronto</th>\n",
       "      <th>Montreal</th>\n",
       "      <th>Vancouver</th>\n",
       "      <th>Ottawa</th>\n",
       "      <th>Halifax</th>\n",
       "      <th>Calgary</th>\n",
       "      <th>Winnipeg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>grill_cheese</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sushi</td>\n",
       "      <td>bacon</td>\n",
       "      <td>pita</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>burrito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>condiment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>condiment</td>\n",
       "      <td>burrito</td>\n",
       "      <td>meatless</td>\n",
       "      <td>latte</td>\n",
       "      <td>hearty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>dessert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>grill_cheese</td>\n",
       "      <td>condiment</td>\n",
       "      <td>pizza</td>\n",
       "      <td>burger</td>\n",
       "      <td>burger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>burger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gourmet</td>\n",
       "      <td>grill_cheese</td>\n",
       "      <td>porridge</td>\n",
       "      <td>bacon</td>\n",
       "      <td>mac_cheese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>bacon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dessert</td>\n",
       "      <td>nachos</td>\n",
       "      <td>donut</td>\n",
       "      <td>perogie</td>\n",
       "      <td>condiment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Toronto Montreal     Vancouver        Ottawa   Halifax    Calgary  \\\n",
       "0  grill_cheese      NaN         sushi         bacon      pita  apple_pie   \n",
       "1     condiment      NaN     condiment       burrito  meatless      latte   \n",
       "2       dessert      NaN  grill_cheese     condiment     pizza     burger   \n",
       "3        burger      NaN       gourmet  grill_cheese  porridge      bacon   \n",
       "4         bacon      NaN       dessert        nachos     donut    perogie   \n",
       "\n",
       "     Winnipeg  \n",
       "0     burrito  \n",
       "1      hearty  \n",
       "2      burger  \n",
       "3  mac_cheese  \n",
       "4   condiment  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = ['Toronto', 'Montreal', 'Vancouver', 'Ottawa', \n",
    "          'Halifax', 'Calgary', 'Winnipeg']\n",
    "\n",
    "# Function to get the equivalent of a given word and city, for the other cities above.\n",
    "def city_analogies(word, city2):\n",
    "    \"\"\"Find the top five words that are to a city as word is to city2\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(columns = cities)\n",
    "    for city in cities:\n",
    "        if city == city2:\n",
    "            continue\n",
    "        sims = [res[0] for res in w2v_model.wv.most_similar(positive=[city, word], negative=[city2], topn=5)]\n",
    "        df[city] = sims\n",
    "    return df\n",
    "\n",
    "# What are the other cities' equivalent of poutine?\n",
    "city_analogies('poutine', 'Montreal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Toronto</th>\n",
       "      <th>Montreal</th>\n",
       "      <th>Vancouver</th>\n",
       "      <th>Ottawa</th>\n",
       "      <th>Halifax</th>\n",
       "      <th>Calgary</th>\n",
       "      <th>Winnipeg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Leonard_Cohen</td>\n",
       "      <td>rapper</td>\n",
       "      <td>Celine_Dion</td>\n",
       "      <td>N_S</td>\n",
       "      <td>Taylor_Swift</td>\n",
       "      <td>Man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>celine</td>\n",
       "      <td>Swift</td>\n",
       "      <td>Classified</td>\n",
       "      <td>Kinley</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>Taylor_Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>drake</td>\n",
       "      <td>Bob_Dylan</td>\n",
       "      <td>Weeknd</td>\n",
       "      <td>Classified</td>\n",
       "      <td>Jay_Z</td>\n",
       "      <td>drake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Celine_Dion</td>\n",
       "      <td>Taylor_Swift</td>\n",
       "      <td>Alanis_Morissette</td>\n",
       "      <td>lower_sackville</td>\n",
       "      <td>drake</td>\n",
       "      <td>Janet_Jackson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rapper</td>\n",
       "      <td>Weeknd</td>\n",
       "      <td>drake</td>\n",
       "      <td>Kentville</td>\n",
       "      <td>Weeknd</td>\n",
       "      <td>hip_hop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Toronto       Montreal     Vancouver             Ottawa          Halifax  \\\n",
       "0     NaN  Leonard_Cohen        rapper        Celine_Dion              N_S   \n",
       "1     NaN         celine         Swift         Classified           Kinley   \n",
       "2     NaN          drake     Bob_Dylan             Weeknd       Classified   \n",
       "3     NaN    Celine_Dion  Taylor_Swift  Alanis_Morissette  lower_sackville   \n",
       "4     NaN         rapper        Weeknd              drake        Kentville   \n",
       "\n",
       "        Calgary       Winnipeg  \n",
       "0  Taylor_Swift            Man  \n",
       "1      Thriller   Taylor_Swift  \n",
       "2         Jay_Z          drake  \n",
       "3         drake  Janet_Jackson  \n",
       "4        Weeknd        hip_hop  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What about Drake and Toronto?\n",
    "\n",
    "city_analogies('Drake', 'Toronto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Toronto</th>\n",
       "      <th>Montreal</th>\n",
       "      <th>Vancouver</th>\n",
       "      <th>Ottawa</th>\n",
       "      <th>Halifax</th>\n",
       "      <th>Calgary</th>\n",
       "      <th>Winnipeg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>extravaganza</td>\n",
       "      <td>Old_Port</td>\n",
       "      <td>extravaganza</td>\n",
       "      <td>Parliament_Hill</td>\n",
       "      <td>tall_ship</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the_Forks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Harbourfront_Centre</td>\n",
       "      <td>fete</td>\n",
       "      <td>Harbourfront_Centre</td>\n",
       "      <td>Canada_Day</td>\n",
       "      <td>Yacht_Club</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gimli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>th_edition</td>\n",
       "      <td>extravaganza</td>\n",
       "      <td>Wine_Festival</td>\n",
       "      <td>th_edition</td>\n",
       "      <td>Citadel_Hill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Voyageur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>junos</td>\n",
       "      <td>th_edition</td>\n",
       "      <td>festival</td>\n",
       "      <td>Rideau_Canal</td>\n",
       "      <td>Eastlink_Centre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Handsome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>telecast</td>\n",
       "      <td>festival</td>\n",
       "      <td>th_edition</td>\n",
       "      <td>Rideau_Hall</td>\n",
       "      <td>Lunenburg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stampede</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Toronto      Montreal            Vancouver           Ottawa  \\\n",
       "0         extravaganza      Old_Port         extravaganza  Parliament_Hill   \n",
       "1  Harbourfront_Centre          fete  Harbourfront_Centre       Canada_Day   \n",
       "2           th_edition  extravaganza        Wine_Festival       th_edition   \n",
       "3                junos    th_edition             festival     Rideau_Canal   \n",
       "4             telecast      festival           th_edition      Rideau_Hall   \n",
       "\n",
       "           Halifax Calgary   Winnipeg  \n",
       "0        tall_ship     NaN  the_Forks  \n",
       "1       Yacht_Club     NaN      Gimli  \n",
       "2     Citadel_Hill     NaN   Voyageur  \n",
       "3  Eastlink_Centre     NaN   Handsome  \n",
       "4        Lunenburg     NaN   Stampede  "
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Calgary Stampede?\n",
    "\n",
    "city_analogies('Calgary_Stampede', 'Calgary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any gender biases hidden in CBC stories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('software_engineer', 0.47878366708755493),\n",
       " ('programmer', 0.45141080021858215),\n",
       " ('coder', 0.4328458905220032),\n",
       " ('illustrator', 0.4310019314289093),\n",
       " ('designer', 0.4219575524330139),\n",
       " ('software_developer', 0.4206082224845886)]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['woman','computer_programmer'], negative=['man'], topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('president_ceo', 0.6393610239028931),\n",
       " ('chief_executive', 0.6163231134414673),\n",
       " ('vice_president', 0.558792769908905),\n",
       " ('interim_ceo', 0.5408685207366943),\n",
       " ('executive_director', 0.5326038002967834),\n",
       " ('president', 0.5301772356033325)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['woman','ceo'], negative=['man'], topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. What about the reverse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paramedic', 0.5126073956489563),\n",
       " ('correction_officer', 0.49881601333618164),\n",
       " ('doctor', 0.4887685179710388),\n",
       " ('doctor_nurse', 0.4882134795188904),\n",
       " ('correctional_officer', 0.4696846604347229),\n",
       " ('jail_guard', 0.46581488847732544)]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['man','nurse'], negative=['woman'], topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Harper and Trudeau governments\n",
    "\n",
    "Are there differences in word associations in different governments? I trained the same model on 60,000 stories each, one during the Harper years and another during a comparable span of the Trudeau admnistration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59904\n",
      "60154\n"
     ]
    }
   ],
   "source": [
    "# For transparency, here is how the data was divided by date.\n",
    "\n",
    "harper = archive[archive.pubdate <= '2015-10-20'].body.dropna().tolist()\n",
    "trudeau = archive[(archive.pubdate >= '2015-10-21') & (archive.pubdate <= '2017-10-01')].body.dropna().tolist()\n",
    "\n",
    "print(len(harper))\n",
    "print(len(trudeau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oil_pipeline', 0.7849512696266174),\n",
       " ('pipeline_project', 0.6998418569564819),\n",
       " ('Keystone_XL', 0.6799078583717346),\n",
       " ('Trans_Mountain', 0.6449182033538818),\n",
       " ('Energy_East', 0.6427786946296692),\n",
       " ('pipeline_expansion', 0.6376286745071411),\n",
       " ('TransCanada', 0.6276552677154541),\n",
       " ('Kinder_Morgan', 0.5928761959075928),\n",
       " ('kilometre_pipeline', 0.5828524827957153),\n",
       " ('Dakota_Access', 0.5747994184494019)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trudeau_model.wv.most_similar('pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oil_pipeline', 0.7015752792358398),\n",
       " ('TransCanada', 0.6530065536499023),\n",
       " ('pipeline_project', 0.6472420692443848),\n",
       " ('Pipeline', 0.6225021481513977),\n",
       " ('Keystone_XL', 0.6192481517791748),\n",
       " ('pipeline_carry', 0.6157941818237305),\n",
       " ('gas_pipeline', 0.5762631893157959),\n",
       " ('oilsand', 0.5699912309646606),\n",
       " ('oil_sand', 0.569394588470459),\n",
       " ('enbridge', 0.567440390586853)]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harper_model.wv.most_similar('pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('civil_war', 0.5777413845062256),\n",
       " ('War', 0.5279940366744995),\n",
       " ('jihad', 0.5268000364303589),\n",
       " ('enemy', 0.507621705532074),\n",
       " ('Kurds', 0.49582090973854065),\n",
       " ('battlefield', 0.4905870854854584),\n",
       " ('Rwanda', 0.4848124086856842),\n",
       " ('Syrian_civil', 0.4839516282081604),\n",
       " ('Yugoslavia', 0.4831847548484802),\n",
       " ('Soviet_Union', 0.4819676876068115)]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trudeau_model.wv.most_similar('war')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conflict', 0.564337432384491),\n",
       " ('war_Syria', 0.5504180192947388),\n",
       " ('jihad', 0.5458228588104248),\n",
       " ('War', 0.5448305010795593),\n",
       " ('civil_war', 0.5056760311126709),\n",
       " ('military_mission', 0.4953949451446533),\n",
       " ('First_World', 0.4877474308013916),\n",
       " ('Afghanistan', 0.4829769730567932),\n",
       " ('battlefield', 0.4757120609283447),\n",
       " ('insurgency', 0.47483381628990173)]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harper_model.wv.most_similar('war')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the model\n",
    "\n",
    "You can export your model and visualize it on https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts import word2vec2tensor\n",
    "\n",
    "with open('cbc_tensor.tsv', 'w+') as file_vector:\n",
    "    with open('cbc_tensor_metadata.tsv', 'w+') as file_metadata:\n",
    "        for word in w2v_model.wv.index2word:\n",
    "            file_metadata.write(gensim.utils.to_unicode(word) + gensim.utils.to_unicode('\\n'))\n",
    "            vector_row = '\\t'.join(str(x) for x in w2train_modelsel[word])\n",
    "            file_vector.write(vector_row + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
